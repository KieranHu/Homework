---
title: "HW03"
author: "Kaiyuan Hu"
date: "2017/4/18"
output: pdf_document
---

# Question 2

## a)

```{r}
library(MASS)
data(Animals)
plot(log(Animals$body),log(Animals$brain),type = 'n')
text(log(Animals$body),log(Animals$brain),row.names(Animals),cex=.6)
```

There seems to be a linear trend on the double-log scale. The three most obvious outliers are *Brachiosaurus*, *Triceratops*, and *Dipliodocus*, since they have smaller brain sizes corresponding to their large body sizes.


## b)

```{r}
model = lm(log(brain)~log(body), data = Animals)
model_wo=lm(log(brain)~log(body), data=Animals[-c(6,16,26),])
plot(log(Animals$body),log(Animals$brain),type = 'n')
text(log(Animals$body),log(Animals$brain),row.names(Animals),cex=.7)
abline(model,col='red', lty=2)
abline(model_wo,col='green', lty=2)
legend('topleft',c('Model1 (with outliers)','Model1 (without outliers)'),col=c('red','green'),lty=2)
```

```{r}
# The coefficients of Model1 with outliers
modelcoef <- c(exp(model$coefficients[1]),model$coefficients[2])
names(modelcoef) <- c('K','r')
modelcoef
```

```{r}
#The coefficients of Model1 without outliers
modelwcoef <- c(exp(model_wo$coefficients[1]),model_wo$coefficients[2])
names(modelwcoef) <- c('K','r')
modelwcoef
```

## c)

$$
EQ = \frac{Brain}{\hat{Brain}} = \frac{\exp(\log Brain)}{\exp(\log \hat{Brain})} = \exp(\log Brain - \log \hat{Brain}) = \exp(e_i)
$$

## d)

```{r}
result = sort(Animals$brain/exp(predict(model_wo,Animals)),T)
result
```

EQ of the outlier rank lower. 

------------------------------------------------------------------

# Question 3

## a)

```{r}
data(biopsy)
biopsy <- na.omit(biopsy)
X <-  as.matrix(biopsy[,2:10])
colnames(X) <- c('F1','F2','F3','F4','F5','F6','F7','F8','F9')
head(X)
```

## b)

```{r}
P <- prcomp(X,center=T,scale. = F)
Xbar <- X-P$center
colnames(Xbar) <- c('F1_bar','F2_bar','F3_bar','F4_bar','F5_bar','F6_bar','F7_bar','F8_bar','F9_bar')
head(Xbar)
```

## c)

```{r}
pairs(P$x,col=c('blue','red')[biopsy$class],pch=20)
screeplot(P)
```

The first component explains the majority of variance.

## d)

```{r}
# I The 1st principal component is doing a weighted sum over all features.
P$rotation[,1]

# II F6 has the largest loading for the 2nd principal component.
names(which.max(abs(P$rotation[,2])))

# III F1 has the largest loading for the 3rd principal component.
names(which.max(abs(P$rotation[,3])))
```

## e)

```{r}
B <-  matrix(rep(0,81),9,9)
for (i in 1:9) B[i,i]=1
B[,9] <- rep(1/9,9)
W <-  X%*%B
head(W)
```

Columns 1,6,9.

## f)

```{r}
library(glmnet)
set.seed(3)
lasso <- cv.glmnet(W,biopsy$class,family='binomial',alpha=1)
coef(lasso,s=lasso$lambda.1se)
```

*lambda.1se* is the value of $\lambda$ in the list has error within 1 standard error of the best model. $\alpha=1$ is only LASSO constraint. Column 1,6,9 are important features. This result is same as principal componets.

## g)

```{r}
library(AUC)
set.seed(3)
lasso2 <- cv.glmnet(X,biopsy$class,family='binomial',alpha=1)
coef(lasso2,s=lasso2$lambda.1se)
pred1 <- predict(lasso,W,s=lasso$lambda.1se)
pred2 <- predict(lasso2,X,s=lasso2$lambda.1se)
plot(pred1,pred2)
abline(0,1)
auc(roc(pred1,biopsy$class))
auc(roc(pred2,biopsy$class))
```

Those two model are really similar.

## h)

### I)
$\bar S$ is the most important feature used in this model.

### II)

```{r}
obs <- biopsy
obs <- obs[rowSums(obs[,c(2:10)]==10)>0,]
summary(obs$class)

```

### III)

```{r}
S_bar <- W[,9]
l <- list()
for (i in 1:9) l[[i]] <- S_bar[biopsy[,i+1]==10]
boxplot(l)
abline(h=quantile(S_bar[biopsy$class=='benign'],c(1:3)/4),col='blue')
abline(h=quantile(S_bar[biopsy$class=='malignant'],c(1:3)/4),col='red')
legend('bottomright',c('benign','malignant'),col=c('blue','red'),lty = 1)
```

As we can see, *benign* and *malignant* are important and highly correlated to our prediction. So this is the reason why the model in part (f) select those two features. 

----------------------------------------------------------