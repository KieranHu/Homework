---
title: "CSC465HW01"
author: "Kaiyuan Hu"
date: "2017/2/12"
output: pdf_document
header-includes:
- \usepackage{amsmath}
---

# Question 1

## a)
Adding a constant *c* in each response *Y* means we can rewite the original model as $Y+C = \beta_0  + \beta_1 X+C$, which is $Y+C = (\beta_0 + C) + \beta_1 X$. So, the new $\beta_1$ is equal to the previous one and th new $\beta_0$ is equal to the original $\beta_0 + c$.


## b)
For the new model: 
$$ \hat Y + C = \hat \beta_0^{'} + \hat \beta_1^{'} X_{new}$$
$$\hat Y = \hat \beta_0^{'} + \hat \beta_1{'} X_{new} - C$$
$$ \hat \beta_1{'} X_{new} - C = \hat \beta_1 X$$
$$ \hat \beta_1{'} X_{new}  = \hat \beta_1 X + C$$
$$ X_{new} = (\hat \beta_1 X + C)/ \hat \beta_1{'}$$
-----------------------------------------------------------------------------------------------

# Question 2

## a)
$$trace(AB^T) = \sum_{i = 1}^{n}\sum_{j =1}^{m}a_{ij}b_{ji} = \sum_{j =1}^{m}\sum_{i = 1}^{n}b_{ji}a_{ij} = trace(B^T A)$$
Then:
$$trace(H) = trace(X(X^T X)^{-1}X^T) = trace(X^T X(X^T X)^{-1}) = trace(I_{q*q}) = q$$

## b) 

### I)
$$H*H = X(X^T X)^{-1}X^T * X(X^T X)^{-1}X^T = X(X^T X)^{-1}X^T X(X^T X)^{-1}X^T = X(X^TX)^{-1}*I*X^T = H $$
So matrix $H$ is idempotent.

### II)
We assume $H \neq HH$, which means there exist $n$ such that $H^n y$ is the closet point in $S_q$ closet to $y$. This statement is contradict to the fact that $Hy$ is the closet point in $S_q$ closet to $y$. So, $H = HH$. 

## c)

### I)
$$\mathbf{H^{'}} = \left[\begin{array}
{rrr}
1/n & \cdots & 1/n \\
\cdots & \cdots & \cdots \\
1/n & \cdots & 1/n
\end{array}\right]_{n*n}$$
$trace(H^{'}) = 1$

### II)
$$\mathbf{H^{'}} = \left[\begin{array}
{rrrrrr}
1/2 & 1/2 & 0 & 0 & 0 & \cdots \\
1/3 & 1/3 & 1/3 & 0 & 0 & \cdots \\
0 & 1/3 & 1/3 & 1/3 & 0 & \cdots \\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots\\
0 & 0 & \cdots & 1/3 & 1/3 & 1/3 \\
0 & 0 & 0 & \cdots & 1/2 & 1/2
\end{array}\right]$$
$trace(H^{'}) = 1 + (n-2)/3$

### III)
$$\mathbf{H^{'}} = \left[\begin{array}
{rrrr}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\cdots & \cdots & \cdots & \cdots\\
0 & \cdots & 0 & 1
\end{array}\right]_{n*n}$$
$trace(H^{'}) = n$

## d)
When $n = 2$, the order of complexity is $I = II < III$
When $n > 2$, the order of complexity is $I < II < III$

The degree of freedom of $I$ is 1, so the model complexity is low. For model $II$, as the complexity increase the $MSE$ will decrease. For model $III$, the $MSE$ is zero, this model is really complexity. 

----------------------------------------------------------------------------------------------

# Question 3

```{r}
library(MASS)
```

## a)

```{r}
modela = lm(bwt~age, data = birthwt)
```

## b) 

```{r}
inf_measure = influence.measures(modela)
```

## c)

### I&II&III&IV&V)
```{r}
plot(birthwt$bwt~birthwt$age, pch = 20, xlab = 'age', ylab = 'bwt') # I
abline(modela, col = 'red') # II
flag = inf_measure$infmat
points(birthwt$age, birthwt$bwt, col = ifelse(flag[,6] > 4/dim(birthwt)[1], 'red', 'black'), pch = 20) # III
```

```{r}
plot(birthwt$bwt~birthwt$age, pch = 20, xlab = 'age', ylab = 'bwt') # I
abline(modela, col = 'red')
points(birthwt$age, birthwt$bwt, col = ifelse(flag[,3] < 0, 'blue', 'green'), pch = 2)
```

```{r}
plot(birthwt$bwt~birthwt$age, pch = 20, xlab = 'age', ylab = 'bwt') # I
abline(modela, col = 'red')# IV
points(birthwt$age, birthwt$bwt, col = ifelse(flag[,4] < 1, 'blue', 'green'), pch = 6) #V
```

## d)
It seems the fitted line distinguishes high and low *DFFITSi*. High *DFFITSi* value usually above the fitted line and low *DFFITSi* value usually below the fitted line. 

## e)
No. As we can see in the figure, no flag criterion is dominates. We need them all to detect outliers.

## f)
As we can see in the figure, If the bwt value is higher than 4000 or lower than 2000, observations will have high covariance ratio.

If a value is too far away from most other values, remove this point, the standard error of $\beta$ will decrease. 

## g)
$X_{,,}$ yield lower varience, since $(X_{,,}^TX_{,,})^{-1} < (X_{,}^TX_{,})^{-1}$.

## h)
The large sample variance will cause small beta varience. 

## i)
No. Coveriance ratio it alone can not tell which point is really a outlier, since coveriance ratio only compute the distance between points. However, far distance does not necessaily means it is a outlier. 

------------------------------------------------------------------------------------------------

# Question 4

## a)

```{r}
newbirthwt = subset(birthwt, flag[, 3] > 0)
newbirthwt$smoke = as.factor(newbirthwt$smoke)
```

## b)

```{r}
summary.table = function(fit){
    Nonsmoke.Intercept = summary(fit)$coef[1,1]
    Nonsmoke.slope = 0
    Smoke.Intercept = summary(fit)$coef[2,1] + Nonsmoke.Intercept
    Smoke.slope = 0
    
    if(i == 1){
      Nonsmoke.slope = summary(fit)$coef[2,1]
      Smoke.Intercept = NA
      Smoke.slope = NA
    }
    
    if(i == 3){
      Smoke.Intercept = Smoke.slope = summary(fit)$coef[3,1]
    }
    
    if(i >= 4 && i<= 5){
      Nonsmoke.slope = summary(fit)$coef[3,1]
      Smoke.slope = summary(fit)$coef[4,1]
    }
    
    if(i == 6){
      Nonsmoke.slope = summary(fit)$coef[3,1]
      Smoke.slope = Nonsmoke.slope + summary(fit)$coef[4,1]
    }
    
    Radj = summary(fit)$adj.r.squared
    Numerator.d.f = summary(fit)$f[2]
    Denominator.d.f = summary(fit)$f[3]
    Fstat = summary(fit)$f[1]
    Pv = 1 - pf(Fstat,Numerator.d.f,Denominator.d.f)
  
    sumname = data.frame( Nonsmoke.Intercept, Nonsmoke.slope, Smoke.Intercept, Smoke.slope,
                          Radj, Numerator.d.f, Denominator.d.f, Fstat, Pv)
    sum = c( Nonsmoke.Intercept, Nonsmoke.slope, Smoke.Intercept, Smoke.slope, Radj,
             Numerator.d.f,Denominator.d.f, Fstat, Pv)
  
    names(sum) = names(sumname)
    
    # print(summary(fit))
  return(sum)
}

newbirthwt$nonsmoke <- ifelse(newbirthwt$smoke == 1, 0, 1)
newbirthwt$smoke <- as.factor(newbirthwt$smoke)
newbirthwt$nonsmoke<- as.factor(newbirthwt$nonsmoke)
bwt <- newbirthwt$bwt
smoke <- newbirthwt$smoke
nonsmoke <- newbirthwt$nonsmoke
age <- newbirthwt$age

formula.list = list(
  bwt ~ age,
  bwt ~ smoke,
  bwt ~ smoke + age,
  bwt ~ smoke + smoke:age,
  bwt ~ smoke + nonsmoke:age,
  bwt ~ smoke * age
)

table = NULL
for (i in 1:6)  table = rbind(table, summary.table(lm(formula.list[[i]]))) 
row.names(table) = formula.list 
table
```

## c)
```{r}
par(mfrow=c(3,2))
newbirthwt$smoke <- as.numeric(newbirthwt$smoke)
newbirthwt$nonsmoke <- ifelse(newbirthwt$smoke == 1, 0, 1)
newbirthwt$smoke <- as.factor(newbirthwt$smoke)
newbirthwt$nonsmoke<- as.factor(newbirthwt$nonsmoke)
bwt <- newbirthwt$bwt
smoke <- newbirthwt$smoke
nonsmoke <- newbirthwt$nonsmoke
age <- newbirthwt$age

formula.list = list(
  bwt ~ age,
  bwt ~ smoke,
  bwt ~ smoke + age,
  bwt ~ smoke + smoke:age,
  bwt ~ smoke + nonsmoke:age,
  bwt ~ smoke * age
)
data = list(
data.frame(age),
data.frame(smoke),
data.frame(smoke, age),
data.frame(smoke, smoke:age),
data.frame(smoke, nonsmoke:age),
data.frame(smoke * age))


for (i in 1:6) {
  plot(predict(lm(formula.list[[i]]), newdata = data[i]), age, 
       xlab = "bwt", ylab = "age",main= formula.list[[i]])
}
```

## d)

The model *bwt ~ smoke \* age*, *bwt ~ smoke + smoke:age*, and *bwt ~ smoke + nonsmoke:age*have the highest $R^2_{adj}$ value, which is 0.121. Actually those three models are three identical models.

```{r}
summary(lm(bwt ~ smoke * age))
```
For nonsmoker the model is $Y = 2873.681 + 29.077 * X$.
For smoker the model is $Y = 3543.187 - 5.53 * X$.

## e)

Since model4, model5 and model6 are three identical models. So, I only consider model1, model2 and model3 and compare them with model6.

Among model1, model2 and model3, model3 has the largest number of model degree of freedom. 

```{r}
M6 = lm(bwt ~ smoke * age)
M3 = lm(bwt ~ smoke + age)
anova(M3,M6)
```
As we can the p-value in the anvoa, the full model is slightly better than the model3, which yield the same conclusion as the $R^2_{adj}$ ranking. 

